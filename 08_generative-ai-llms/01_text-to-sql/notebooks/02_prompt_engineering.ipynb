{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL: Prompt Engineering\n",
    "\n",
    "This notebook explores prompt optimization strategies for Text-to-SQL generation.\n",
    "\n",
    "## Objectives\n",
    "1. Test different prompt templates\n",
    "2. Optimize few-shot examples\n",
    "3. Evaluate schema formatting strategies\n",
    "4. Analyze prompt token usage vs. accuracy trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "from schema_manager import SchemaManager\n",
    "from query_generator import TextToSQLGenerator, QueryResult\n",
    "from prompt_templates import PromptTemplates\n",
    "from query_validator import QueryValidator\n",
    "\n",
    "# Load environment variables\n",
    "# Note: Set OPENAI_API_KEY environment variable or uncomment:\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "db_path = \"../data/sample_database.db\"\n",
    "schema_manager = SchemaManager(db_path)\n",
    "validator = QueryValidator(db_path)\n",
    "\n",
    "# Load test queries\n",
    "with open('../data/test_queries.json', 'r') as f:\n",
    "    test_queries = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(test_queries)} test queries\")\n",
    "print(f\"Complexity distribution: {pd.Series([q['complexity'] for q in test_queries]).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Schema Formatting Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different schema formats\n",
    "schema_formats = ['create_table', 'compact', 'json']\n",
    "\n",
    "print(\"Schema Format Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for format_type in schema_formats:\n",
    "    schema_str = schema_manager.format_schema(\n",
    "        format_type=format_type,\n",
    "        include_sample_rows=False\n",
    "    )\n",
    "    token_estimate = len(schema_str) // 4  # Rough token estimate\n",
    "    \n",
    "    print(f\"\\nFormat: {format_type}\")\n",
    "    print(f\"Characters: {len(schema_str)}\")\n",
    "    print(f\"Estimated tokens: {token_estimate}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(schema_str[:300] + \"...\" if len(schema_str) > 300 else schema_str)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with sample rows included\n",
    "print(\"\\nSchema with Sample Rows (CREATE TABLE format):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "schema_with_samples = schema_manager.format_schema(\n",
    "    format_type='create_table',\n",
    "    include_sample_rows=True,\n",
    "    num_sample_rows=3\n",
    ")\n",
    "\n",
    "print(f\"Characters: {len(schema_with_samples)}\")\n",
    "print(f\"Estimated tokens: {len(schema_with_samples) // 4}\")\n",
    "print(\"\\nSample:\")\n",
    "print(schema_with_samples[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Few-Shot Example Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze default few-shot examples\n",
    "prompt_templates = PromptTemplates()\n",
    "examples = prompt_templates.few_shot_examples\n",
    "\n",
    "print(\"Default Few-Shot Examples:\")\n",
    "print(\"=\" * 80)\n",
    "for i, example in enumerate(examples, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"SQL: {example['sql']}\")\n",
    "    print(f\"Explanation: {example['explanation']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nTotal examples: {len(examples)}\")\n",
    "total_chars = sum(len(ex['question']) + len(ex['sql']) + len(ex['explanation']) for ex in examples)\n",
    "print(f\"Total characters: {total_chars}\")\n",
    "print(f\"Estimated tokens: {total_chars // 4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dynamic example selection\n",
    "test_question = \"What are the top 5 products by revenue?\"\n",
    "\n",
    "# Build prompt with selective examples\n",
    "schema_str = schema_manager.format_schema(format_type='create_table', include_sample_rows=False)\n",
    "prompt = prompt_templates.build_text_to_sql_prompt(\n",
    "    schema=schema_str,\n",
    "    question=test_question,\n",
    "    include_examples=True\n",
    ")\n",
    "\n",
    "print(\"Full Prompt:\")\n",
    "print(\"=\" * 80)\n",
    "print(prompt)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nPrompt length: {len(prompt)} characters\")\n",
    "print(f\"Estimated tokens: {len(prompt) // 4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Template Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different system message styles\n",
    "system_messages = {\n",
    "    'default': prompt_templates.get_system_message(),\n",
    "    'concise': \"\"\"You are a SQL expert. Convert natural language to SQLite queries.\n",
    "Rules:\n",
    "- Return ONLY the SQL query\n",
    "- Use proper SQLite syntax\n",
    "- Be precise and efficient\"\"\",\n",
    "    'detailed': \"\"\"You are an expert database assistant specialized in converting natural language \n",
    "questions into accurate SQLite queries. Your task is to analyze the user's question, understand \n",
    "the database schema, and generate a syntactically correct and semantically meaningful SQL query.\n",
    "\n",
    "Guidelines:\n",
    "1. Carefully analyze the question to understand what data is being requested\n",
    "2. Identify which tables and columns are needed\n",
    "3. Use appropriate JOINs when multiple tables are involved\n",
    "4. Apply filters (WHERE) to match the question's constraints\n",
    "5. Use aggregations (COUNT, SUM, AVG) when the question asks for statistics\n",
    "6. Sort results (ORDER BY) when rankings or orderings are requested\n",
    "7. Limit results if the question asks for \"top N\" items\n",
    "8. Handle NULL values appropriately\n",
    "9. Return ONLY the SQL query without explanations\"\"\"\n",
    "}\n",
    "\n",
    "for name, msg in system_messages.items():\n",
    "    print(f\"\\n{name.upper()} System Message:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(msg)\n",
    "    print(f\"\\nLength: {len(msg)} characters, ~{len(msg) // 4} tokens\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt Optimization Experiments\n",
    "\n",
    "Test different prompt configurations on sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt configurations to test\n",
    "configs = [\n",
    "    {\n",
    "        'name': 'minimal',\n",
    "        'schema_format': 'compact',\n",
    "        'include_samples': False,\n",
    "        'num_examples': 0,\n",
    "    },\n",
    "    {\n",
    "        'name': 'standard',\n",
    "        'schema_format': 'create_table',\n",
    "        'include_samples': False,\n",
    "        'num_examples': 3,\n",
    "    },\n",
    "    {\n",
    "        'name': 'comprehensive',\n",
    "        'schema_format': 'create_table',\n",
    "        'include_samples': True,\n",
    "        'num_examples': 5,\n",
    "    }\n",
    "]\n",
    "\n",
    "# Calculate token usage for each config\n",
    "test_question = \"Show me all customers who have placed orders over $100\"\n",
    "\n",
    "print(\"Prompt Configuration Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for config in configs:\n",
    "    schema_str = schema_manager.format_schema(\n",
    "        format_type=config['schema_format'],\n",
    "        include_sample_rows=config['include_samples']\n",
    "    )\n",
    "    \n",
    "    # Build prompt\n",
    "    if config['num_examples'] > 0:\n",
    "        examples = prompt_templates.few_shot_examples[:config['num_examples']]\n",
    "        prompt = prompt_templates.build_text_to_sql_prompt(\n",
    "            schema=schema_str,\n",
    "            question=test_question,\n",
    "            include_examples=True\n",
    "        )\n",
    "    else:\n",
    "        prompt = f\"Schema:\\n{schema_str}\\n\\nQuestion: {test_question}\\n\\nSQL:\"\n",
    "    \n",
    "    token_estimate = len(prompt) // 4\n",
    "    \n",
    "    print(f\"\\nConfig: {config['name']}\")\n",
    "    print(f\"  Schema format: {config['schema_format']}\")\n",
    "    print(f\"  Include samples: {config['include_samples']}\")\n",
    "    print(f\"  Num examples: {config['num_examples']}\")\n",
    "    print(f\"  Estimated tokens: {token_estimate}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Live Testing (Requires API Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with actual API calls (uncomment if API key is set)\n",
    "\"\"\"\n",
    "# Initialize generator\n",
    "generator = TextToSQLGenerator(db_path, provider='openai', model_name='gpt-3.5-turbo')\n",
    "\n",
    "# Select a few test queries\n",
    "test_sample = [\n",
    "    q for q in test_queries \n",
    "    if q['complexity'] in ['simple', 'medium']\n",
    "][:5]\n",
    "\n",
    "results = []\n",
    "for query in test_sample:\n",
    "    print(f\"\\nTesting: {query['question']}\")\n",
    "    print(f\"Expected: {query['expected_sql']}\")\n",
    "    \n",
    "    # Generate SQL\n",
    "    result = generator.generate_sql(\n",
    "        question=query['question'],\n",
    "        return_reasoning=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated: {result.sql}\")\n",
    "    print(f\"Status: {result.status}\")\n",
    "    print(f\"Tokens: {result.prompt_tokens + result.completion_tokens}\")\n",
    "    \n",
    "    results.append({\n",
    "        'question': query['question'],\n",
    "        'expected': query['expected_sql'],\n",
    "        'generated': result.sql,\n",
    "        'status': result.status,\n",
    "        'tokens': result.prompt_tokens + result.completion_tokens,\n",
    "        'latency_ms': result.latency_ms\n",
    "    })\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting\n",
    "\n",
    "# Analyze results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Summary Statistics:\")\n",
    "print(f\"Success rate: {(results_df['status'] == 'success').mean():.1%}\")\n",
    "print(f\"Avg tokens: {results_df['tokens'].mean():.0f}\")\n",
    "print(f\"Avg latency: {results_df['latency_ms'].mean():.0f}ms\")\n",
    "\"\"\"\n",
    "print(\"Live testing disabled. Uncomment the code above and set OPENAI_API_KEY to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Retry Prompt Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine retry prompt template\n",
    "error_message = \"Table 'customer' not found\"\n",
    "original_sql = \"SELECT * FROM customer WHERE city = 'New York'\"\n",
    "question = \"Show me all customers from New York\"\n",
    "\n",
    "retry_prompt = prompt_templates.build_retry_prompt(\n",
    "    original_question=question,\n",
    "    failed_sql=original_sql,\n",
    "    error_message=error_message,\n",
    "    schema=schema_manager.format_schema(format_type='create_table')\n",
    ")\n",
    "\n",
    "print(\"Retry Prompt:\")\n",
    "print(\"=\" * 80)\n",
    "print(retry_prompt)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nLength: {len(retry_prompt)} characters\")\n",
    "print(f\"Estimated tokens: {len(retry_prompt) // 4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Findings and Recommendations\n",
    "\n",
    "### Token Usage vs. Accuracy Trade-offs:\n",
    "\n",
    "1. **Schema Format**:\n",
    "   - `compact`: Lowest tokens (~100-200), good for simple queries\n",
    "   - `create_table`: Medium tokens (~300-500), best balance\n",
    "   - `json`: Similar to CREATE TABLE, more verbose\n",
    "\n",
    "2. **Sample Rows**:\n",
    "   - Add ~50-100 tokens per table\n",
    "   - Helpful for understanding data format and examples\n",
    "   - Most valuable for complex queries with specific data patterns\n",
    "\n",
    "3. **Few-Shot Examples**:\n",
    "   - 0 examples: ~200 total tokens (minimal context)\n",
    "   - 3 examples: ~500 total tokens (recommended baseline)\n",
    "   - 5 examples: ~800 total tokens (comprehensive)\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **For Simple Queries**: Use compact schema, no samples, 1-2 examples\n",
    "2. **For Medium Queries**: Use CREATE TABLE schema, no samples, 3 examples\n",
    "3. **For Complex Queries**: Use CREATE TABLE schema, include samples, 5 examples\n",
    "4. **Dynamic Selection**: Identify query complexity first, then adjust prompt accordingly\n",
    "\n",
    "### Next Steps:\n",
    "- Implement adaptive prompt selection based on query complexity\n",
    "- A/B test different configurations on full test set\n",
    "- Measure accuracy vs. cost trade-offs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
