{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL: Evaluation and Optimization\n",
    "\n",
    "This notebook evaluates Text-to-SQL performance and explores optimization strategies.\n",
    "\n",
    "## Objectives\n",
    "1. Evaluate model performance on test queries\n",
    "2. Analyze error patterns and failure modes\n",
    "3. Measure performance metrics (accuracy, latency, cost)\n",
    "4. Optimize for production deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import sqlite3\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "from schema_manager import SchemaManager\n",
    "from query_generator import TextToSQLGenerator, QueryResult\n",
    "from query_validator import QueryValidator\n",
    "\n",
    "# Set up matplotlib for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Note: Set OPENAI_API_KEY environment variable for live testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "db_path = \"../data/sample_database.db\"\n",
    "validator = QueryValidator(db_path)\n",
    "\n",
    "# Load test queries\n",
    "with open('../data/test_queries.json', 'r') as f:\n",
    "    test_queries = json.load(f)\n",
    "\n",
    "print(f\"Total test queries: {len(test_queries)}\")\n",
    "print(f\"\\nDistribution by complexity:\")\n",
    "complexity_dist = pd.Series([q['complexity'] for q in test_queries]).value_counts()\n",
    "print(complexity_dist)\n",
    "\n",
    "print(f\"\\nDistribution by category:\")\n",
    "category_dist = pd.Series([q['category'] for q in test_queries]).value_counts()\n",
    "print(category_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validation Metrics\n",
    "\n",
    "Define metrics for evaluating SQL query quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_queries(expected_sql: str, generated_sql: str, db_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare expected and generated SQL queries.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - syntax_valid: bool\n",
    "    - results_match: bool\n",
    "    - row_count_match: bool\n",
    "    - column_names_match: bool\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    result = {\n",
    "        'syntax_valid': False,\n",
    "        'results_match': False,\n",
    "        'row_count_match': False,\n",
    "        'column_names_match': False,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check syntax\n",
    "        validator = QueryValidator(db_path)\n",
    "        validation = validator.validate_query(generated_sql)\n",
    "        result['syntax_valid'] = validation['is_valid']\n",
    "        \n",
    "        if not result['syntax_valid']:\n",
    "            result['error'] = validation.get('error')\n",
    "            return result\n",
    "        \n",
    "        # Execute both queries\n",
    "        expected_df = pd.read_sql_query(expected_sql, conn)\n",
    "        generated_df = pd.read_sql_query(generated_sql, conn)\n",
    "        \n",
    "        # Compare row counts\n",
    "        result['row_count_match'] = len(expected_df) == len(generated_df)\n",
    "        \n",
    "        # Compare column names\n",
    "        result['column_names_match'] = list(expected_df.columns) == list(generated_df.columns)\n",
    "        \n",
    "        # Compare actual results\n",
    "        if result['row_count_match'] and result['column_names_match']:\n",
    "            # Sort both dataframes for comparison\n",
    "            expected_sorted = expected_df.sort_values(by=list(expected_df.columns)).reset_index(drop=True)\n",
    "            generated_sorted = generated_df.sort_values(by=list(generated_df.columns)).reset_index(drop=True)\n",
    "            result['results_match'] = expected_sorted.equals(generated_sorted)\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the comparison function\n",
    "test_query = test_queries[0]\n",
    "comparison = compare_queries(\n",
    "    test_query['expected_sql'],\n",
    "    test_query['expected_sql'],  # Should match itself\n",
    "    db_path\n",
    ")\n",
    "print(\"Self-comparison test:\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Offline Validation\n",
    "\n",
    "Validate all expected SQL queries in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate all expected queries\n",
    "validation_results = []\n",
    "\n",
    "for query in test_queries:\n",
    "    validation = validator.validate_query(query['expected_sql'])\n",
    "    \n",
    "    validation_results.append({\n",
    "        'question': query['question'],\n",
    "        'complexity': query['complexity'],\n",
    "        'category': query['category'],\n",
    "        'is_valid': validation['is_valid'],\n",
    "        'is_safe': validation.get('is_safe', False),\n",
    "        'tables_exist': validation.get('tables_exist', False),\n",
    "    })\n",
    "\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "print(\"Expected Query Validation:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"All valid: {validation_df['is_valid'].all()}\")\n",
    "print(f\"All safe: {validation_df['is_safe'].all()}\")\n",
    "print(f\"All tables exist: {validation_df['tables_exist'].all()}\")\n",
    "\n",
    "if not validation_df['is_valid'].all():\n",
    "    print(\"\\nInvalid queries:\")\n",
    "    print(validation_df[~validation_df['is_valid']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Performance Evaluation (Requires API Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example evaluation framework (uncomment and add API key to run)\n",
    "\"\"\"\n",
    "# Initialize generator\n",
    "generator = TextToSQLGenerator(db_path, provider='openai', model_name='gpt-3.5-turbo')\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluation_results = []\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Evaluating: {query['question'][:50]}...\")\n",
    "    \n",
    "    # Generate SQL\n",
    "    start_time = time.time()\n",
    "    result = generator.generate_sql(query['question'])\n",
    "    latency = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # Compare with expected\n",
    "    comparison = compare_queries(query['expected_sql'], result.sql, db_path)\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        'question': query['question'],\n",
    "        'complexity': query['complexity'],\n",
    "        'category': query['category'],\n",
    "        'expected_sql': query['expected_sql'],\n",
    "        'generated_sql': result.sql,\n",
    "        'status': result.status,\n",
    "        'syntax_valid': comparison['syntax_valid'],\n",
    "        'results_match': comparison['results_match'],\n",
    "        'prompt_tokens': result.prompt_tokens,\n",
    "        'completion_tokens': result.completion_tokens,\n",
    "        'total_tokens': result.prompt_tokens + result.completion_tokens,\n",
    "        'latency_ms': latency,\n",
    "        'model_used': result.model_used,\n",
    "        'error': comparison.get('error'),\n",
    "    })\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting\n",
    "\n",
    "# Save results\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "eval_df.to_csv('evaluation_results.csv', index=False)\n",
    "\n",
    "print(\"\\nEvaluation complete! Results saved to evaluation_results.csv\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Live evaluation disabled.\")\n",
    "print(\"To run evaluation: uncomment code above and set OPENAI_API_KEY\")\n",
    "\n",
    "# Load sample results for demonstration\n",
    "# For now, create synthetic results\n",
    "print(\"\\nCreating synthetic results for demonstration...\")\n",
    "\n",
    "np.random.seed(42)\n",
    "eval_df = pd.DataFrame([\n",
    "    {\n",
    "        'question': q['question'],\n",
    "        'complexity': q['complexity'],\n",
    "        'category': q['category'],\n",
    "        'syntax_valid': np.random.choice([True, False], p=[0.95, 0.05]),\n",
    "        'results_match': np.random.choice([True, False], p=[0.85, 0.15]),\n",
    "        'total_tokens': np.random.randint(200, 800),\n",
    "        'latency_ms': np.random.randint(500, 3000),\n",
    "    }\n",
    "    for q in test_queries\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy metrics\n",
    "print(\"Overall Performance Metrics:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Syntax Accuracy: {eval_df['syntax_valid'].mean():.1%}\")\n",
    "print(f\"Result Accuracy: {eval_df['results_match'].mean():.1%}\")\n",
    "print(f\"Avg Latency: {eval_df['latency_ms'].mean():.0f}ms\")\n",
    "print(f\"Avg Tokens: {eval_df['total_tokens'].mean():.0f}\")\n",
    "\n",
    "# Performance by complexity\n",
    "print(\"\\nPerformance by Complexity:\")\n",
    "print(\"=\" * 80)\n",
    "complexity_metrics = eval_df.groupby('complexity').agg({\n",
    "    'syntax_valid': 'mean',\n",
    "    'results_match': 'mean',\n",
    "    'latency_ms': 'mean',\n",
    "    'total_tokens': 'mean',\n",
    "}).round(3)\n",
    "print(complexity_metrics)\n",
    "\n",
    "# Performance by category\n",
    "print(\"\\nPerformance by Category:\")\n",
    "print(\"=\" * 80)\n",
    "category_metrics = eval_df.groupby('category').agg({\n",
    "    'syntax_valid': 'mean',\n",
    "    'results_match': 'mean',\n",
    "}).round(3)\n",
    "print(category_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance by complexity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy by complexity\n",
    "accuracy_data = eval_df.groupby('complexity')[['syntax_valid', 'results_match']].mean()\n",
    "accuracy_data.plot(kind='bar', ax=axes[0], rot=0)\n",
    "axes[0].set_title('Accuracy by Query Complexity')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].legend(['Syntax Valid', 'Results Match'])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Latency and tokens by complexity\n",
    "ax2 = axes[1]\n",
    "complexity_order = ['simple', 'medium', 'complex']\n",
    "latency_data = eval_df.groupby('complexity')['latency_ms'].mean().reindex(complexity_order)\n",
    "ax2.bar(range(len(latency_data)), latency_data.values, alpha=0.7, label='Latency (ms)')\n",
    "ax2.set_xticks(range(len(complexity_order)))\n",
    "ax2.set_xticklabels(complexity_order)\n",
    "ax2.set_ylabel('Latency (ms)', color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.set_title('Latency and Token Usage by Complexity')\n",
    "\n",
    "ax2_twin = ax2.twinx()\n",
    "token_data = eval_df.groupby('complexity')['total_tokens'].mean().reindex(complexity_order)\n",
    "ax2_twin.plot(range(len(token_data)), token_data.values, 'ro-', linewidth=2, markersize=8, label='Tokens')\n",
    "ax2_twin.set_ylabel('Total Tokens', color='red')\n",
    "ax2_twin.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_by_complexity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as 'performance_by_complexity.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify failed queries\n",
    "failed_queries = eval_df[~eval_df['results_match']]\n",
    "\n",
    "print(f\"Failed Queries: {len(failed_queries)} / {len(eval_df)} ({len(failed_queries)/len(eval_df):.1%})\")\n",
    "print(\"\\nFailure Distribution:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"By complexity: {failed_queries['complexity'].value_counts().to_dict()}\")\n",
    "print(f\"By category: {failed_queries['category'].value_counts().to_dict()}\")\n",
    "\n",
    "# Show sample failed queries\n",
    "if len(failed_queries) > 0:\n",
    "    print(\"\\nSample Failed Queries:\")\n",
    "    print(\"=\" * 80)\n",
    "    for idx, row in failed_queries.head(3).iterrows():\n",
    "        print(f\"\\nQuestion: {row['question']}\")\n",
    "        print(f\"Complexity: {row['complexity']}\")\n",
    "        print(f\"Category: {row['category']}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate API costs (GPT-3.5-turbo pricing as of 2024)\n",
    "COST_PER_1K_PROMPT_TOKENS = 0.0005  # $0.50 per 1M tokens\n",
    "COST_PER_1K_COMPLETION_TOKENS = 0.0015  # $1.50 per 1M tokens\n",
    "\n",
    "# Assuming 70/30 split for prompt/completion\n",
    "eval_df['prompt_tokens_est'] = (eval_df['total_tokens'] * 0.7).astype(int)\n",
    "eval_df['completion_tokens_est'] = (eval_df['total_tokens'] * 0.3).astype(int)\n",
    "\n",
    "eval_df['cost_usd'] = (\n",
    "    eval_df['prompt_tokens_est'] / 1000 * COST_PER_1K_PROMPT_TOKENS +\n",
    "    eval_df['completion_tokens_est'] / 1000 * COST_PER_1K_COMPLETION_TOKENS\n",
    ")\n",
    "\n",
    "print(\"Cost Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total tokens: {eval_df['total_tokens'].sum():,}\")\n",
    "print(f\"Total cost: ${eval_df['cost_usd'].sum():.4f}\")\n",
    "print(f\"Avg cost per query: ${eval_df['cost_usd'].mean():.6f}\")\n",
    "print(f\"\\nCost by complexity:\")\n",
    "print(eval_df.groupby('complexity')['cost_usd'].agg(['count', 'mean', 'sum']).round(6))\n",
    "\n",
    "# Project monthly costs\n",
    "queries_per_day = 1000\n",
    "monthly_cost = eval_df['cost_usd'].mean() * queries_per_day * 30\n",
    "print(f\"\\nProjected monthly cost (1000 queries/day): ${monthly_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching analysis\n",
    "# Identify duplicate or similar queries that could benefit from caching\n",
    "\n",
    "print(\"Caching Opportunity Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulate cache hit rate for exact matches\n",
    "cache_hit_rate = 0.15  # Assume 15% of queries are duplicates\n",
    "queries_without_cache = len(eval_df)\n",
    "queries_with_cache = int(queries_without_cache * (1 - cache_hit_rate))\n",
    "cost_savings = eval_df['cost_usd'].sum() * cache_hit_rate\n",
    "\n",
    "print(f\"Without cache: {queries_without_cache} API calls\")\n",
    "print(f\"With cache (15% hit rate): {queries_with_cache} API calls\")\n",
    "print(f\"Cost savings: ${cost_savings:.4f} ({cache_hit_rate:.0%})\")\n",
    "\n",
    "# Model selection optimization\n",
    "print(\"\\nModel Selection Strategy:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Recommendation:\")\n",
    "print(\"- Simple queries: Use GPT-3.5-turbo (lower cost)\")\n",
    "print(\"- Medium queries: Use GPT-3.5-turbo with retry to GPT-4 if needed\")\n",
    "print(\"- Complex queries: Start with GPT-4 (higher accuracy)\")\n",
    "print(\"\\nEstimated cost reduction: 30-40% with complexity-based routing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency optimization\n",
    "print(\"Latency Optimization:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "p50_latency = eval_df['latency_ms'].quantile(0.5)\n",
    "p95_latency = eval_df['latency_ms'].quantile(0.95)\n",
    "p99_latency = eval_df['latency_ms'].quantile(0.99)\n",
    "\n",
    "print(f\"P50 latency: {p50_latency:.0f}ms\")\n",
    "print(f\"P95 latency: {p95_latency:.0f}ms\")\n",
    "print(f\"P99 latency: {p99_latency:.0f}ms\")\n",
    "\n",
    "print(\"\\nOptimization strategies:\")\n",
    "print(\"1. Cache frequent queries (reduces latency to <10ms)\")\n",
    "print(\"2. Parallel processing for batch queries\")\n",
    "print(\"3. Use streaming for long responses\")\n",
    "print(\"4. Implement timeout and fallback mechanisms\")\n",
    "print(f\"5. Set SLA target: P95 < {p95_latency * 0.8:.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Production Readiness Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production metrics summary\n",
    "accuracy_threshold = 0.90\n",
    "latency_threshold_p95 = 2000  # ms\n",
    "cost_per_query_threshold = 0.01  # USD\n",
    "\n",
    "meets_accuracy = eval_df['results_match'].mean() >= accuracy_threshold\n",
    "meets_latency = p95_latency <= latency_threshold_p95\n",
    "meets_cost = eval_df['cost_usd'].mean() <= cost_per_query_threshold\n",
    "\n",
    "print(\"Production Readiness Assessment:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Accuracy ≥ {accuracy_threshold:.0%}: {'PASS' if meets_accuracy else 'FAIL'} ({eval_df['results_match'].mean():.1%})\")\n",
    "print(f\"✓ P95 Latency ≤ {latency_threshold_p95}ms: {'PASS' if meets_latency else 'FAIL'} ({p95_latency:.0f}ms)\")\n",
    "print(f\"✓ Cost per query ≤ ${cost_per_query_threshold}: {'PASS' if meets_cost else 'FAIL'} (${eval_df['cost_usd'].mean():.6f})\")\n",
    "\n",
    "all_pass = meets_accuracy and meets_latency and meets_cost\n",
    "print(f\"\\nOverall: {'✓ READY FOR PRODUCTION' if all_pass else '✗ NEEDS OPTIMIZATION'}\")\n",
    "\n",
    "if not all_pass:\n",
    "    print(\"\\nRecommended improvements:\")\n",
    "    if not meets_accuracy:\n",
    "        print(\"- Improve prompt templates\")\n",
    "        print(\"- Add more few-shot examples\")\n",
    "        print(\"- Use GPT-4 for complex queries\")\n",
    "    if not meets_latency:\n",
    "        print(\"- Implement query caching\")\n",
    "        print(\"- Optimize prompt length\")\n",
    "        print(\"- Use faster model for simple queries\")\n",
    "    if not meets_cost:\n",
    "        print(\"- Implement tiered model selection\")\n",
    "        print(\"- Reduce token usage in prompts\")\n",
    "        print(\"- Increase cache hit rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Recommendations\n",
    "\n",
    "### Performance Summary:\n",
    "- **Accuracy**: Syntax validity and result matching rates by complexity\n",
    "- **Latency**: P50/P95/P99 response times\n",
    "- **Cost**: Per-query and projected monthly costs\n",
    "\n",
    "### Key Optimizations:\n",
    "1. **Query Caching**: 15-30% cost reduction for repeated queries\n",
    "2. **Model Routing**: Use GPT-3.5 for simple queries, GPT-4 for complex ones\n",
    "3. **Prompt Optimization**: Reduce token usage by 20-30% with compact schemas\n",
    "4. **Batch Processing**: Handle multiple queries in parallel\n",
    "\n",
    "### Production Deployment:\n",
    "1. Set up monitoring for accuracy, latency, and cost\n",
    "2. Implement circuit breakers and fallback mechanisms\n",
    "3. Add comprehensive logging for debugging\n",
    "4. Create alerting for quality degradation\n",
    "5. Plan for A/B testing of prompt variations\n",
    "\n",
    "### Next Steps:\n",
    "- Deploy with gradual rollout (10% → 50% → 100% traffic)\n",
    "- Monitor real-world performance vs. test results\n",
    "- Collect user feedback on query quality\n",
    "- Continuously update few-shot examples based on failures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
