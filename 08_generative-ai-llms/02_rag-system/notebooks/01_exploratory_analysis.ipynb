{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System - Exploratory Analysis\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the structure of our knowledge base\n",
    "- Explore different chunking strategies\n",
    "- Analyze document statistics\n",
    "- Test embedding quality\n",
    "\n",
    "**For Interviews:**  \n",
    "This notebook demonstrates your ability to:\n",
    "- Analyze data before building ML systems\n",
    "- Make informed decisions about hyperparameters\n",
    "- Understand trade-offs in RAG design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Knowledge Base\n",
    "\n",
    "First, let's understand what data we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load knowledge base\n",
    "with open('../data/knowledge_base.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "print(f\"üìö Loaded {len(documents)} documents\\n\")\n",
    "\n",
    "# Show first document\n",
    "print(\"Example Document:\")\n",
    "print(json.dumps(documents[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(documents)\n",
    "\n",
    "# Add word counts\n",
    "df['word_count'] = df['content'].apply(lambda x: len(x.split()))\n",
    "df['char_count'] = df['content'].apply(len)\n",
    "\n",
    "print(\"\\nüìä Document Statistics:\")\n",
    "print(df[['word_count', 'char_count']].describe())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize document lengths\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Word count distribution\n",
    "axes[0].hist(df['word_count'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Distribution of Document Lengths (Words)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Word Count')\n",
    "axes[0].set_ylabel('Number of Documents')\n",
    "axes[0].axvline(df['word_count'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"word_count\"].mean():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Category distribution (if present)\n",
    "if 'category' in df.columns:\n",
    "    category_counts = df['category'].value_counts()\n",
    "    axes[1].barh(category_counts.index, category_counts.values)\n",
    "    axes[1].set_title('Documents by Category', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Number of Documents')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No category information', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Average document length: {df['word_count'].mean():.1f} words\")\n",
    "print(f\"üìà Median document length: {df['word_count'].median():.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunking Strategy Analysis\n",
    "\n",
    "**Interview Key Point:** Chunking is crucial in RAG. Too small = lost context, too large = irrelevant info.\n",
    "\n",
    "Let's test different chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document_processor import chunk_text, process_documents\n",
    "\n",
    "# Test different chunk sizes\n",
    "chunk_sizes = [100, 200, 400, 800]\n",
    "overlap = 50\n",
    "\n",
    "chunk_analysis = []\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    chunks = process_documents(documents, chunk_size=chunk_size, overlap=overlap)\n",
    "    \n",
    "    chunk_analysis.append({\n",
    "        'chunk_size': chunk_size,\n",
    "        'total_chunks': len(chunks),\n",
    "        'avg_chunks_per_doc': len(chunks) / len(documents),\n",
    "        'avg_chunk_words': sum(len(c['text'].split()) for c in chunks) / len(chunks)\n",
    "    })\n",
    "\n",
    "chunk_df = pd.DataFrame(chunk_analysis)\n",
    "print(\"\\nüîç Chunking Strategy Comparison:\")\n",
    "print(chunk_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize chunking impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total chunks created\n",
    "axes[0].plot(chunk_df['chunk_size'], chunk_df['total_chunks'], marker='o', linewidth=2, markersize=8)\n",
    "axes[0].set_title('Total Chunks vs Chunk Size', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Chunk Size (words)')\n",
    "axes[0].set_ylabel('Total Number of Chunks')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average chunks per document\n",
    "axes[1].plot(chunk_df['chunk_size'], chunk_df['avg_chunks_per_doc'], marker='s', linewidth=2, markersize=8, color='green')\n",
    "axes[1].set_title('Average Chunks per Document', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Chunk Size (words)')\n",
    "axes[1].set_ylabel('Chunks per Document')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Insights\n",
    "\n",
    "**For Interviews - Be ready to discuss:**\n",
    "1. **Smaller chunks (100-200 words):**\n",
    "   - ‚úÖ More precise retrieval\n",
    "   - ‚ùå May lose context\n",
    "   - Good for: Factual Q&A\n",
    "\n",
    "2. **Medium chunks (400 words):**\n",
    "   - ‚úÖ Balance of precision and context\n",
    "   - ‚úÖ Industry standard for many use cases\n",
    "   - Good for: General RAG applications\n",
    "\n",
    "3. **Large chunks (800+ words):**\n",
    "   - ‚úÖ More context preserved\n",
    "   - ‚ùå May include irrelevant information\n",
    "   - Good for: Complex topics requiring background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overlap Impact Analysis\n",
    "\n",
    "**Interview Key Point:** Overlap prevents information loss at chunk boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different overlap values\n",
    "chunk_size = 300  # Fixed chunk size\n",
    "overlaps = [0, 25, 50, 100]\n",
    "\n",
    "overlap_analysis = []\n",
    "\n",
    "for overlap in overlaps:\n",
    "    chunks = process_documents(documents, chunk_size=chunk_size, overlap=overlap)\n",
    "    \n",
    "    overlap_analysis.append({\n",
    "        'overlap': overlap,\n",
    "        'total_chunks': len(chunks),\n",
    "        'overlap_percentage': (overlap / chunk_size) * 100\n",
    "    })\n",
    "\n",
    "overlap_df = pd.DataFrame(overlap_analysis)\n",
    "print(\"\\nüîÑ Overlap Strategy Comparison (chunk_size=300):\")\n",
    "print(overlap_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overlap impact\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.bar(overlap_df['overlap'].astype(str), overlap_df['total_chunks'], edgecolor='black', alpha=0.7)\n",
    "ax.set_title('Impact of Overlap on Chunk Count', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Overlap (words)')\n",
    "ax.set_ylabel('Total Number of Chunks')\n",
    "\n",
    "# Add percentage labels\n",
    "for i, row in overlap_df.iterrows():\n",
    "    ax.text(i, row['total_chunks'] + 2, f\"{row['overlap_percentage']:.1f}%\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insight: More overlap creates more chunks (and more storage/compute cost)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example: Analyze Chunk Boundaries\n",
    "\n",
    "Let's look at actual chunks to understand overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunks with overlap\n",
    "chunks = process_documents(documents[:1], chunk_size=50, overlap=10)\n",
    "\n",
    "print(f\"\\nüìÑ Document chunks with overlap=10:\")\n",
    "print(f\"Total chunks created: {len(chunks)}\\n\")\n",
    "\n",
    "# Show first 3 chunks\n",
    "for i in range(min(3, len(chunks))):\n",
    "    chunk = chunks[i]\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    print(f\"ID: {chunk['id']}\")\n",
    "    print(f\"Text: {chunk['text'][:200]}...\")\n",
    "    print(f\"Words: {len(chunk['text'].split())}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recommended Configuration\n",
    "\n",
    "Based on our analysis, here are recommended starting points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚öôÔ∏è RECOMMENDED RAG CONFIGURATIONS\\n\")\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        'use_case': 'Factual Q&A',\n",
    "        'chunk_size': 200,\n",
    "        'overlap': 30,\n",
    "        'top_k': 3,\n",
    "        'reasoning': 'Short chunks for precise fact retrieval'\n",
    "    },\n",
    "    {\n",
    "        'use_case': 'General Purpose',\n",
    "        'chunk_size': 400,\n",
    "        'overlap': 50,\n",
    "        'top_k': 3,\n",
    "        'reasoning': 'Balanced context and precision'\n",
    "    },\n",
    "    {\n",
    "        'use_case': 'Complex Topics',\n",
    "        'chunk_size': 600,\n",
    "        'overlap': 75,\n",
    "        'top_k': 2,\n",
    "        'reasoning': 'More context per chunk, fewer chunks needed'\n",
    "    }\n",
    "]\n",
    "\n",
    "config_df = pd.DataFrame(configs)\n",
    "print(config_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Interview Tip: Always explain WHY you chose these parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Statistics Analysis\n",
    "\n",
    "Understanding vocabulary and text complexity helps inform chunk size decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary\n",
    "all_words = []\n",
    "for doc in documents:\n",
    "    all_words.extend(doc['content'].lower().split())\n",
    "\n",
    "unique_words = set(all_words)\n",
    "\n",
    "print(\"\\nüìö Vocabulary Statistics:\")\n",
    "print(f\"Total words: {len(all_words):,}\")\n",
    "print(f\"Unique words: {len(unique_words):,}\")\n",
    "print(f\"Vocabulary richness: {len(unique_words)/len(all_words):.2%}\")\n",
    "\n",
    "# Average sentence length\n",
    "sentence_lengths = []\n",
    "for doc in documents:\n",
    "    sentences = doc['content'].split('.')\n",
    "    for sent in sentences:\n",
    "        if sent.strip():\n",
    "            sentence_lengths.append(len(sent.split()))\n",
    "\n",
    "print(f\"\\nAverage sentence length: {sum(sentence_lengths)/len(sentence_lengths):.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways for Interviews\n",
    "\n",
    "**When discussing RAG in interviews, mention:**\n",
    "\n",
    "1. **Data Analysis First:**\n",
    "   - Always analyze document statistics before choosing chunk size\n",
    "   - Consider average document length, vocabulary, domain\n",
    "\n",
    "2. **Chunking Trade-offs:**\n",
    "   - Chunk size affects retrieval precision vs context\n",
    "   - Overlap prevents information loss but increases cost\n",
    "   - No one-size-fits-all solution\n",
    "\n",
    "3. **Iterative Approach:**\n",
    "   - Start with recommended defaults (chunk_size=400, overlap=50)\n",
    "   - Evaluate retrieval quality\n",
    "   - Adjust based on results\n",
    "\n",
    "4. **Cost Considerations:**\n",
    "   - More chunks = more storage + more embedding cost\n",
    "   - Larger top_k = more tokens sent to LLM = higher cost\n",
    "   - Balance quality vs cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚úÖ Exploratory Analysis Complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Implement RAG pipeline (see 02_rag_implementation.ipynb)\")\n",
    "print(\"2. Evaluate retrieval quality (see 03_evaluation_optimization.ipynb)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
