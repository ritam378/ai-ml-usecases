{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System - Implementation Walkthrough\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Build a complete RAG pipeline from scratch\n",
    "- Understand each component (chunking, embedding, retrieval, generation)\n",
    "- Test the system with real queries\n",
    "- Learn best practices for production RAG\n",
    "\n",
    "**For Interviews:**  \n",
    "This notebook shows you can:\n",
    "- Implement RAG systems end-to-end\n",
    "- Explain design decisions\n",
    "- Debug and test ML systems\n",
    "- Think about production considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from document_processor import load_documents, process_documents\n",
    "from vector_store import VectorStore\n",
    "from rag_pipeline import RAGPipeline\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Document Processing\n",
    "\n",
    "**Interview Key Point:** RAG starts with good document processing. Garbage in = garbage out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load knowledge base\n",
    "print(\"üìö Loading Knowledge Base...\\n\")\n",
    "documents = load_documents('../data/knowledge_base.json')\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"\\nFirst document preview:\")\n",
    "print(f\"  ID: {documents[0]['id']}\")\n",
    "print(f\"  Title: {documents[0]['title']}\")\n",
    "print(f\"  Category: {documents[0]['category']}\")\n",
    "print(f\"  Content length: {len(documents[0]['content'].split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Strategy\n",
    "\n",
    "**Why chunking matters:**\n",
    "- Embeddings work better on coherent chunks (not entire documents)\n",
    "- Retrieval is more precise with smaller chunks\n",
    "- LLM context windows are limited\n",
    "\n",
    "**Common strategy:** ~400 words with ~50 word overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process documents into chunks\n",
    "print(\"‚úÇÔ∏è Chunking Documents...\\n\")\n",
    "\n",
    "CHUNK_SIZE = 400  # words per chunk\n",
    "OVERLAP = 50      # overlap between chunks\n",
    "\n",
    "chunks = process_documents(\n",
    "    documents,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    overlap=OVERLAP\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Chunking Results:\")\n",
    "print(f\"  Total chunks: {len(chunks)}\")\n",
    "print(f\"  Average chunks per document: {len(chunks)/len(documents):.1f}\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE} words\")\n",
    "print(f\"  Overlap: {OVERLAP} words ({OVERLAP/CHUNK_SIZE*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a chunk\n",
    "print(\"\\nüîç Example Chunk:\\n\")\n",
    "example_chunk = chunks[0]\n",
    "print(f\"ID: {example_chunk['id']}\")\n",
    "print(f\"Source: {example_chunk['source_title']}\")\n",
    "print(f\"Chunk {example_chunk['chunk_index'] + 1} of {example_chunk['total_chunks']}\")\n",
    "print(f\"\\nText:\\n{example_chunk['text'][:300]}...\")\n",
    "print(f\"\\nWord count: {len(example_chunk['text'].split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Store Setup\n",
    "\n",
    "**Interview Key Point:** Vector databases store embeddings for semantic search.\n",
    "\n",
    "**How it works:**\n",
    "1. Text ‚Üí Embedding model ‚Üí Vector (list of numbers)\n",
    "2. Store vectors in database with metadata\n",
    "3. Query ‚Üí Embedding ‚Üí Find nearest vectors (cosine similarity)\n",
    "4. Return corresponding text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "print(\"üóÑÔ∏è Initializing Vector Store...\\n\")\n",
    "\n",
    "vector_store = VectorStore(\n",
    "    collection_name=\"rag_demo_kb\",\n",
    "    persist_directory=\"../chroma_db\"\n",
    ")\n",
    "\n",
    "print(f\"Collection: {vector_store.collection_name}\")\n",
    "print(f\"Current document count: {vector_store.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add chunks to vector store\n",
    "# This will:\n",
    "# 1. Generate embeddings for each chunk (using sentence-transformers)\n",
    "# 2. Store embeddings + metadata in ChromaDB\n",
    "# 3. Index for fast similarity search\n",
    "\n",
    "print(\"\\nüì• Adding chunks to vector store...\")\n",
    "print(\"(This may take a minute on first run to download the embedding model)\\n\")\n",
    "\n",
    "vector_store.add_documents(chunks)\n",
    "\n",
    "print(f\"\\n‚úì Vector store ready!\")\n",
    "print(f\"  Total chunks indexed: {vector_store.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Semantic Search\n",
    "\n",
    "**This is the \"Retrieval\" in RAG**\n",
    "\n",
    "Let's test the semantic search capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"What is Python programming?\"\n",
    "\n",
    "print(f\"üîç Query: '{test_query}'\\n\")\n",
    "print(\"Searching knowledge base...\\n\")\n",
    "\n",
    "results = vector_store.query(test_query, top_k=3)\n",
    "\n",
    "print(f\"Retrieved {len(results['documents'])} chunks:\\n\")\n",
    "\n",
    "for i, (doc, dist, meta) in enumerate(zip(\n",
    "    results['documents'],\n",
    "    results['distances'],\n",
    "    results['metadatas']\n",
    "), 1):\n",
    "    similarity_score = 1 - dist  # Convert distance to similarity\n",
    "    print(f\"--- Result {i} ---\")\n",
    "    print(f\"Similarity: {similarity_score:.3f}\")\n",
    "    print(f\"Source: {meta['source_title']}\")\n",
    "    print(f\"Category: {meta['category']}\")\n",
    "    print(f\"Text: {doc[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Similarity Scores\n",
    "\n",
    "**Interview Tip:** Be ready to explain similarity metrics!\n",
    "\n",
    "- **1.0 = Perfect match** (identical text)\n",
    "- **0.8-0.9 = Very similar** (paraphrases, related concepts)\n",
    "- **0.6-0.7 = Somewhat related** (shares topics)\n",
    "- **< 0.5 = Weakly related** (might not be relevant)\n",
    "\n",
    "ChromaDB uses **L2 distance** by default, which we convert to similarity (1 - distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different queries to see semantic understanding\n",
    "test_queries = [\n",
    "    \"programming languages\",\n",
    "    \"artificial intelligence and machine learning\",\n",
    "    \"web development frameworks\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Semantic Search:\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    results = vector_store.query(query, top_k=1)\n",
    "    \n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Top match: {results['metadatas'][0]['source_title']}\")\n",
    "    print(f\"Similarity: {1 - results['distances'][0]:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Complete RAG Pipeline\n",
    "\n",
    "Now let's use the full RAG pipeline that orchestrates everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG pipeline\n",
    "print(\"üöÄ Initializing RAG Pipeline...\\n\")\n",
    "\n",
    "rag = RAGPipeline(\n",
    "    collection_name=\"rag_demo_complete\",\n",
    "    chunk_size=400,\n",
    "    overlap=50\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "rag.add_documents('../data/knowledge_base.json')\n",
    "\n",
    "print(\"\\n‚úì RAG Pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the RAG System\n",
    "\n",
    "**The RAG workflow:**\n",
    "1. **Retrieve:** Find relevant chunks from knowledge base\n",
    "2. **Augment:** Assemble prompt with retrieved context\n",
    "3. **Generate:** LLM generates answer based on context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "question = \"What is Python and what is it used for?\"\n",
    "\n",
    "print(f\"‚ùì Question: {question}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Query the RAG system\n",
    "response = rag.query(question, top_k=3)\n",
    "\n",
    "# Show the answer\n",
    "print(\"\\nüìù RAG RESPONSE:\\n\")\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the prompt that would be sent to an LLM\n",
    "print(\"\\nüîç PROMPT FOR LLM:\\n\")\n",
    "print(\"=\"*60)\n",
    "print(response['prompt'])\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüí° In production, this prompt would be sent to GPT-4, Claude, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze retrieved contexts\n",
    "print(\"\\nüìö RETRIEVED CONTEXTS:\\n\")\n",
    "\n",
    "for i, (ctx, dist) in enumerate(zip(response['contexts'], response['distances']), 1):\n",
    "    print(f\"--- Context {i} ---\")\n",
    "    print(f\"Similarity: {1-dist:.3f}\")\n",
    "    print(f\"Text: {ctx[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Testing Multiple Queries\n",
    "\n",
    "Let's test the RAG system with various question types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does web development work?\",\n",
    "    \"Explain data structures\",\n",
    "    \"What are the benefits of cloud computing?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing RAG System with Multiple Queries\\n\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"‚ùì {question}\\n\")\n",
    "    \n",
    "    # Get response\n",
    "    response = rag.query(question, top_k=2)\n",
    "    \n",
    "    # Show top retrieved chunk\n",
    "    if response['contexts']:\n",
    "        top_context = response['contexts'][0]\n",
    "        top_similarity = 1 - response['distances'][0]\n",
    "        \n",
    "        print(f\"üìä Top Match (similarity: {top_similarity:.3f}):\")\n",
    "        print(f\"   {top_context[:200]}...\\n\")\n",
    "    else:\n",
    "        print(\"   No relevant context found\\n\")\n",
    "    \n",
    "    print(\"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Understanding RAG Components\n",
    "\n",
    "**For interviews, be ready to explain each component:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèóÔ∏è RAG SYSTEM ARCHITECTURE\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "components = [\n",
    "    {\n",
    "        'component': '1. Document Processor',\n",
    "        'purpose': 'Clean, chunk, and prepare documents',\n",
    "        'key_decision': f'Chunk size = {CHUNK_SIZE}, Overlap = {OVERLAP}',\n",
    "        'why': 'Balance precision vs context'\n",
    "    },\n",
    "    {\n",
    "        'component': '2. Embedding Model',\n",
    "        'purpose': 'Convert text to semantic vectors',\n",
    "        'key_decision': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        'why': 'Fast, good quality, 384 dimensions'\n",
    "    },\n",
    "    {\n",
    "        'component': '3. Vector Store',\n",
    "        'purpose': 'Store & search embeddings efficiently',\n",
    "        'key_decision': 'ChromaDB (in-memory + persistent)',\n",
    "        'why': 'Simple, no external dependencies'\n",
    "    },\n",
    "    {\n",
    "        'component': '4. Retriever',\n",
    "        'purpose': 'Find most similar chunks to query',\n",
    "        'key_decision': 'top_k=3, cosine similarity',\n",
    "        'why': '3 chunks ‚âà 1200 words of context'\n",
    "    },\n",
    "    {\n",
    "        'component': '5. Prompt Template',\n",
    "        'purpose': 'Format context + question for LLM',\n",
    "        'key_decision': 'Clear instructions, cite sources',\n",
    "        'why': 'Reduce hallucinations, improve quality'\n",
    "    }\n",
    "]\n",
    "\n",
    "for comp in components:\n",
    "    print(f\"\\n{comp['component']}\")\n",
    "    print(f\"  Purpose: {comp['purpose']}\")\n",
    "    print(f\"  Decision: {comp['key_decision']}\")\n",
    "    print(f\"  Why: {comp['why']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Production Considerations\n",
    "\n",
    "**Interview talking points for production RAG systems:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè≠ PRODUCTION RAG CHECKLIST\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "considerations = [\n",
    "    {\n",
    "        'category': 'Performance',\n",
    "        'items': [\n",
    "            'Cache embeddings to avoid recomputation',\n",
    "            'Use batch processing for large documents',\n",
    "            'Consider GPU for embedding generation',\n",
    "            'Implement async/parallel processing'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Quality',\n",
    "        'items': [\n",
    "            'Evaluate retrieval precision/recall',\n",
    "            'A/B test different chunk sizes',\n",
    "            'Monitor answer quality metrics',\n",
    "            'Implement user feedback loops'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Scalability',\n",
    "        'items': [\n",
    "            'Use production vector DB (Pinecone, Weaviate, etc.)',\n",
    "            'Implement incremental indexing',\n",
    "            'Plan for millions of documents',\n",
    "            'Monitor memory and disk usage'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Cost',\n",
    "        'items': [\n",
    "            'LLM API costs (per token)',\n",
    "            'Vector DB storage costs',\n",
    "            'Embedding generation costs',\n",
    "            'Trade-off: smaller top_k = cheaper but maybe worse quality'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Security',\n",
    "        'items': [\n",
    "            'Filter sensitive information before indexing',\n",
    "            'Implement access controls on knowledge base',\n",
    "            'Sanitize user queries',\n",
    "            'Audit what information is being retrieved'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for consideration in considerations:\n",
    "    print(f\"\\n{consideration['category']}:\")\n",
    "    for item in consideration['items']:\n",
    "        print(f\"  ‚Ä¢ {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Common RAG Challenges & Solutions\n",
    "\n",
    "**Be ready to discuss these in interviews:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö†Ô∏è COMMON RAG CHALLENGES\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "challenges = [\n",
    "    {\n",
    "        'problem': 'Retrieval returns irrelevant chunks',\n",
    "        'causes': [\n",
    "            'Chunk size too small/large',\n",
    "            'Poor document preprocessing',\n",
    "            'Weak embedding model'\n",
    "        ],\n",
    "        'solutions': [\n",
    "            'Tune chunk size based on evaluation',\n",
    "            'Use better cleaning/normalization',\n",
    "            'Try domain-specific embedding models',\n",
    "            'Implement hybrid search (keyword + semantic)'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'problem': 'LLM ignores retrieved context',\n",
    "        'causes': [\n",
    "            'Context not relevant enough',\n",
    "            'Prompt poorly structured',\n",
    "            'Too much context (information overload)'\n",
    "        ],\n",
    "        'solutions': [\n",
    "            'Improve retrieval quality',\n",
    "            'Use clearer prompt instructions',\n",
    "            'Reduce top_k to most relevant chunks',\n",
    "            'Add explicit grounding instructions'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'problem': 'Hallucinations despite RAG',\n",
    "        'causes': [\n",
    "            'No relevant context found',\n",
    "            'LLM uses pre-training knowledge instead',\n",
    "            'Ambiguous or contradictory context'\n",
    "        ],\n",
    "        'solutions': [\n",
    "            'Explicitly instruct: \"Only use provided context\"',\n",
    "            'Return \"I don\\'t know\" if similarity too low',\n",
    "            'Implement confidence scoring',\n",
    "            'Ask LLM to cite specific context passages'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'problem': 'Slow query response time',\n",
    "        'causes': [\n",
    "            'Embedding generation is slow',\n",
    "            'Vector search not optimized',\n",
    "            'LLM API latency'\n",
    "        ],\n",
    "        'solutions': [\n",
    "            'Cache query embeddings',\n",
    "            'Use approximate nearest neighbor search',\n",
    "            'Stream LLM responses',\n",
    "            'Precompute for common queries'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, challenge in enumerate(challenges, 1):\n",
    "    print(f\"\\n{i}. {challenge['problem']}\")\n",
    "    print(f\"\\n   Causes:\")\n",
    "    for cause in challenge['causes']:\n",
    "        print(f\"   - {cause}\")\n",
    "    print(f\"\\n   Solutions:\")\n",
    "    for solution in challenge['solutions']:\n",
    "        print(f\"   ‚úì {solution}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You've Learned\n",
    "\n",
    "‚úÖ **Technical Skills:**\n",
    "- Built a complete RAG system from scratch\n",
    "- Implemented document chunking strategies\n",
    "- Used vector databases for semantic search\n",
    "- Created production-ready prompts\n",
    "\n",
    "‚úÖ **Interview Ready:**\n",
    "- Can explain RAG architecture\n",
    "- Understand trade-offs (chunk size, top_k, etc.)\n",
    "- Know production considerations\n",
    "- Can debug common RAG problems\n",
    "\n",
    "‚úÖ **Next Steps:**\n",
    "1. Complete evaluation notebook (03_evaluation_optimization.ipynb)\n",
    "2. Practice explaining this system out loud\n",
    "3. Try implementing RAG for a different domain\n",
    "4. Read the interview_questions.md file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "print(\"\\nüßπ Cleaning up...\")\n",
    "rag.vector_store.delete_collection()\n",
    "vector_store.delete_collection()\n",
    "print(\"‚úì Done!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAG IMPLEMENTATION COMPLETE! üéâ\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
