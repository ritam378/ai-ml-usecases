{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System - Evaluation & Optimization\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Measure RAG system quality with metrics\n",
    "- Evaluate retrieval effectiveness\n",
    "- Optimize hyperparameters\n",
    "- Understand trade-offs\n",
    "\n",
    "**For Interviews:**  \n",
    "Shows you can:\n",
    "- Define and measure success metrics\n",
    "- Systematically improve ML systems\n",
    "- Make data-driven decisions\n",
    "- Balance multiple objectives (quality vs cost vs latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from rag_pipeline import RAGPipeline\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: RAG Evaluation Metrics\n",
    "\n",
    "**Interview Key Point:** You can't improve what you don't measure!\n",
    "\n",
    "### Key Metrics for RAG Systems:\n",
    "\n",
    "**Retrieval Metrics:**\n",
    "- **Precision@K:** Of K retrieved chunks, how many are relevant?\n",
    "- **Recall@K:** Of all relevant chunks, how many did we retrieve?\n",
    "- **MRR (Mean Reciprocal Rank):** How high is the first relevant result?\n",
    "\n",
    "**Generation Metrics:**\n",
    "- **Accuracy:** Is the answer correct?\n",
    "- **Groundedness:** Does answer stick to provided context?\n",
    "- **Relevance:** Does answer address the question?\n",
    "- **Completeness:** Is all necessary information included?\n",
    "\n",
    "**System Metrics:**\n",
    "- **Latency:** Time to generate answer\n",
    "- **Cost:** LLM API costs\n",
    "- **Token usage:** Context + generation tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Retrieval Evaluation\n",
    "\n",
    "Let's evaluate how well our retrieval works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG pipeline\n",
    "print(\"ðŸš€ Setting up RAG Pipeline...\\n\")\n",
    "\n",
    "rag = RAGPipeline(\n",
    "    collection_name=\"rag_evaluation\",\n",
    "    chunk_size=400,\n",
    "    overlap=50\n",
    ")\n",
    "\n",
    "# Load knowledge base\n",
    "rag.add_documents('../data/knowledge_base.json')\n",
    "\n",
    "print(\"âœ“ Pipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test questions\n",
    "with open('../data/test_questions.json', 'r') as f:\n",
    "    test_questions = json.load(f)\n",
    "\n",
    "print(f\"ðŸ“ Loaded {len(test_questions)} test questions\\n\")\n",
    "\n",
    "# Show example\n",
    "print(\"Example test question:\")\n",
    "print(json.dumps(test_questions[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate retrieval\n",
    "print(\"\\nðŸ“Š Evaluating Retrieval Quality...\\n\")\n",
    "\n",
    "metrics = rag.evaluate_retrieval('../data/test_questions.json')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETRIEVAL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Precision@3: {metrics['precision@3']:.1%}\")\n",
    "print(f\"Recall@3: {metrics['recall@3']:.1%}\")\n",
    "print(f\"Questions tested: {metrics['num_questions']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(f\"   â€¢ {metrics['precision@3']*100:.0f}% of retrieved chunks were relevant\")\n",
    "print(f\"   â€¢ We found {metrics['recall@3']*100:.0f}% of all relevant information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Precision vs Recall\n",
    "\n",
    "**Interview Tip:** Be ready to explain this trade-off!\n",
    "\n",
    "- **High Precision, Low Recall:** Retrieved chunks are very relevant, but we're missing some good content\n",
    "  - *Solution:* Increase top_k\n",
    "\n",
    "- **Low Precision, High Recall:** We're getting all the relevant content, but also irrelevant chunks\n",
    "  - *Solution:* Decrease top_k, improve chunk quality\n",
    "\n",
    "- **Both Low:** Retrieval isn't working well\n",
    "  - *Solutions:* Better embeddings, different chunk size, hybrid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Hyperparameter Optimization\n",
    "\n",
    "Let's systematically test different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different chunk sizes\n",
    "print(\"ðŸ”¬ Testing Different Chunk Sizes...\\n\")\n",
    "\n",
    "chunk_sizes = [200, 300, 400, 600]\n",
    "results = []\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"Testing chunk_size={chunk_size}...\")\n",
    "    \n",
    "    # Create pipeline with this chunk size\n",
    "    test_rag = RAGPipeline(\n",
    "        collection_name=f\"test_chunk_{chunk_size}\",\n",
    "        chunk_size=chunk_size,\n",
    "        overlap=50\n",
    "    )\n",
    "    \n",
    "    # Add documents\n",
    "    test_rag.add_documents('../data/knowledge_base.json')\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = test_rag.evaluate_retrieval('../data/test_questions.json')\n",
    "    \n",
    "    results.append({\n",
    "        'chunk_size': chunk_size,\n",
    "        'precision': metrics['precision@3'],\n",
    "        'recall': metrics['recall@3'],\n",
    "        'f1_score': 2 * (metrics['precision@3'] * metrics['recall@3']) / \n",
    "                   (metrics['precision@3'] + metrics['recall@3'] + 1e-10),\n",
    "        'total_chunks': test_rag.vector_store.count()\n",
    "    })\n",
    "    \n",
    "    # Cleanup\n",
    "    test_rag.vector_store.delete_collection()\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize chunk size impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Precision & Recall\n",
    "axes[0].plot(results_df['chunk_size'], results_df['precision'], \n",
    "             marker='o', label='Precision@3', linewidth=2)\n",
    "axes[0].plot(results_df['chunk_size'], results_df['recall'], \n",
    "             marker='s', label='Recall@3', linewidth=2)\n",
    "axes[0].plot(results_df['chunk_size'], results_df['f1_score'], \n",
    "             marker='^', label='F1 Score', linewidth=2, linestyle='--')\n",
    "axes[0].set_xlabel('Chunk Size (words)', fontweight='bold')\n",
    "axes[0].set_ylabel('Score', fontweight='bold')\n",
    "axes[0].set_title('Retrieval Quality vs Chunk Size', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Total chunks (cost proxy)\n",
    "axes[1].bar(results_df['chunk_size'].astype(str), results_df['total_chunks'], \n",
    "            edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Chunk Size (words)', fontweight='bold')\n",
    "axes[1].set_ylabel('Total Chunks', fontweight='bold')\n",
    "axes[1].set_title('Storage/Embedding Cost vs Chunk Size', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best chunk size\n",
    "best_idx = results_df['f1_score'].idxmax()\n",
    "best_chunk_size = results_df.loc[best_idx, 'chunk_size']\n",
    "print(f\"\\nâ­ Best chunk size: {best_chunk_size} words (F1={results_df.loc[best_idx, 'f1_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: top_k Optimization\n",
    "\n",
    "**Interview Key Point:** top_k affects quality AND cost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different top_k values\n",
    "print(\"ðŸ”¬ Testing Different top_k Values...\\n\")\n",
    "\n",
    "# Reinitialize with best chunk size\n",
    "eval_rag = RAGPipeline(\n",
    "    collection_name=\"eval_topk\",\n",
    "    chunk_size=400,  # Using default\n",
    "    overlap=50\n",
    ")\n",
    "eval_rag.add_documents('../data/knowledge_base.json')\n",
    "\n",
    "topk_results = []\n",
    "\n",
    "for k in [1, 2, 3, 4, 5]:\n",
    "    print(f\"Testing top_k={k}...\")\n",
    "    \n",
    "    # Manually evaluate with this top_k\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    \n",
    "    for item in test_questions:\n",
    "        question = item['question']\n",
    "        expected_docs = set(item['expected_doc_ids'])\n",
    "        \n",
    "        # Retrieve with this top_k\n",
    "        results = eval_rag.retrieve(question, top_k=k)\n",
    "        \n",
    "        # Get retrieved doc IDs\n",
    "        retrieved_docs = set()\n",
    "        for meta in results['metadatas']:\n",
    "            retrieved_docs.add(meta['source_doc_id'])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        relevant = retrieved_docs & expected_docs\n",
    "        precision = len(relevant) / len(results['ids']) if results['ids'] else 0\n",
    "        recall = len(relevant) / len(expected_docs) if expected_docs else 0\n",
    "        \n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "    \n",
    "    avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "    avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "    \n",
    "    # Estimate cost (tokens sent to LLM)\n",
    "    # Assuming ~400 words per chunk, ~1.3 tokens per word\n",
    "    est_tokens = k * 400 * 1.3\n",
    "    \n",
    "    topk_results.append({\n",
    "        'top_k': k,\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1_score': 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall + 1e-10),\n",
    "        'est_tokens': int(est_tokens)\n",
    "    })\n",
    "\n",
    "topk_df = pd.DataFrame(topk_results)\n",
    "print(\"\\n\" + topk_df.to_string(index=False))\n",
    "\n",
    "# Cleanup\n",
    "eval_rag.vector_store.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top_k trade-offs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Quality metrics\n",
    "axes[0].plot(topk_df['top_k'], topk_df['precision'], marker='o', label='Precision', linewidth=2)\n",
    "axes[0].plot(topk_df['top_k'], topk_df['recall'], marker='s', label='Recall', linewidth=2)\n",
    "axes[0].plot(topk_df['top_k'], topk_df['f1_score'], marker='^', label='F1', linewidth=2, linestyle='--')\n",
    "axes[0].set_xlabel('top_k', fontweight='bold')\n",
    "axes[0].set_ylabel('Score', fontweight='bold')\n",
    "axes[0].set_title('Quality vs top_k', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cost (token usage)\n",
    "axes[1].bar(topk_df['top_k'].astype(str), topk_df['est_tokens'], edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].set_xlabel('top_k', fontweight='bold')\n",
    "axes[1].set_ylabel('Context Tokens', fontweight='bold')\n",
    "axes[1].set_title('LLM Cost vs top_k', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Insight: Higher top_k improves recall but increases cost linearly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Quality-Cost Trade-off Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cost-efficiency score\n",
    "# Higher F1 with fewer tokens = better\n",
    "topk_df['efficiency'] = topk_df['f1_score'] / (topk_df['est_tokens'] / 1000)\n",
    "\n",
    "print(\"\\nðŸ“Š COST-EFFICIENCY ANALYSIS\\n\")\n",
    "print(\"=\"*60)\n",
    "print(topk_df[['top_k', 'f1_score', 'est_tokens', 'efficiency']].to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find sweet spot\n",
    "best_efficiency_idx = topk_df['efficiency'].idxmax()\n",
    "print(f\"\\nâ­ Most cost-efficient: top_k={topk_df.loc[best_efficiency_idx, 'top_k']}\")\n",
    "print(f\"   F1 Score: {topk_df.loc[best_efficiency_idx, 'f1_score']:.3f}\")\n",
    "print(f\"   Est. Tokens: {topk_df.loc[best_efficiency_idx, 'est_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Recommended Configurations\n",
    "\n",
    "Based on our evaluation, here are recommended configs for different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nâš™ï¸ RECOMMENDED CONFIGURATIONS\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        'use_case': 'Cost-Optimized',\n",
    "        'chunk_size': 300,\n",
    "        'overlap': 30,\n",
    "        'top_k': 2,\n",
    "        'when': 'High query volume, tight budget',\n",
    "        'tradeoff': 'Lower quality, minimal cost'\n",
    "    },\n",
    "    {\n",
    "        'use_case': 'Balanced (Recommended)',\n",
    "        'chunk_size': 400,\n",
    "        'overlap': 50,\n",
    "        'top_k': 3,\n",
    "        'when': 'Most production use cases',\n",
    "        'tradeoff': 'Good quality, moderate cost'\n",
    "    },\n",
    "    {\n",
    "        'use_case': 'Quality-Optimized',\n",
    "        'chunk_size': 600,\n",
    "        'overlap': 75,\n",
    "        'top_k': 5,\n",
    "        'when': 'Critical applications, quality > cost',\n",
    "        'tradeoff': 'Best quality, highest cost'\n",
    "    },\n",
    "    {\n",
    "        'use_case': 'Low-Latency',\n",
    "        'chunk_size': 200,\n",
    "        'overlap': 20,\n",
    "        'top_k': 2,\n",
    "        'when': 'Real-time chat, fast response needed',\n",
    "        'tradeoff': 'Faster, but less context'\n",
    "    }\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\n{config['use_case']}:\")\n",
    "    print(f\"  chunk_size: {config['chunk_size']}, overlap: {config['overlap']}, top_k: {config['top_k']}\")\n",
    "    print(f\"  When: {config['when']}\")\n",
    "    print(f\"  Trade-off: {config['tradeoff']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Improvement Strategies\n",
    "\n",
    "**Interview talking points for improving RAG systems:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸš€ RAG OPTIMIZATION STRATEGIES\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "strategies = [\n",
    "    {\n",
    "        'category': 'Better Retrieval',\n",
    "        'techniques': [\n",
    "            'â€¢ Hybrid search (combine keyword BM25 + semantic)',\n",
    "            'â€¢ Reranking: Retrieve top 10, rerank to top 3',\n",
    "            'â€¢ Query expansion (paraphrase user query)',\n",
    "            'â€¢ Domain-specific embedding models',\n",
    "            'â€¢ Metadata filtering (filter by date, category, etc.)'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Better Chunking',\n",
    "        'techniques': [\n",
    "            'â€¢ Semantic chunking (split at topic boundaries)',\n",
    "            'â€¢ Sentence-aware chunking (don\\'t break mid-sentence)',\n",
    "            'â€¢ Parent-child chunking (small chunks + larger context)',\n",
    "            'â€¢ Include chunk context (add doc title to each chunk)'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Better Generation',\n",
    "        'techniques': [\n",
    "            'â€¢ Few-shot examples in prompt',\n",
    "            'â€¢ Chain-of-thought reasoning',\n",
    "            'â€¢ Ask LLM to cite sources',\n",
    "            'â€¢ Self-consistency (generate multiple, pick best)',\n",
    "            'â€¢ Fine-tune LLM on domain data'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'System Improvements',\n",
    "        'techniques': [\n",
    "            'â€¢ Cache frequent queries',\n",
    "            'â€¢ Async/streaming responses',\n",
    "            'â€¢ Monitor and log all queries',\n",
    "            'â€¢ A/B test different configurations',\n",
    "            'â€¢ Collect user feedback for improvement'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\n{strategy['category']}:\")\n",
    "    for technique in strategy['techniques']:\n",
    "        print(f\"  {technique}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Evaluation Best Practices\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "âœ… **Metrics:**\n",
    "- Precision, Recall, F1 for retrieval quality\n",
    "- Cost metrics (tokens, latency)\n",
    "- How to measure quality vs cost trade-offs\n",
    "\n",
    "âœ… **Optimization:**\n",
    "- Systematic hyperparameter tuning\n",
    "- Finding the right chunk size\n",
    "- Balancing top_k for quality and cost\n",
    "\n",
    "âœ… **Interview Ready:**\n",
    "- Can explain evaluation methodology\n",
    "- Understand precision/recall trade-offs\n",
    "- Know multiple improvement strategies\n",
    "- Can discuss production considerations\n",
    "\n",
    "### Key Interview Talking Points:\n",
    "\n",
    "1. **\"How do you evaluate a RAG system?\"**\n",
    "   - Retrieval metrics (precision/recall)\n",
    "   - Generation metrics (accuracy/groundedness)\n",
    "   - System metrics (latency/cost)\n",
    "   - Use test sets with known relevant documents\n",
    "\n",
    "2. **\"How do you optimize chunk size?\"**\n",
    "   - Evaluate on test set with different sizes\n",
    "   - Consider document structure and domain\n",
    "   - Balance precision (small chunks) vs context (large chunks)\n",
    "   - Typical range: 200-600 words\n",
    "\n",
    "3. **\"How do you handle quality vs cost trade-offs?\"**\n",
    "   - Define acceptable quality threshold\n",
    "   - Minimize cost while meeting threshold\n",
    "   - Use smaller top_k for common queries\n",
    "   - Cache results for repeated questions\n",
    "   - Consider streaming for better UX\n",
    "\n",
    "4. **\"How would you improve a poorly performing RAG system?\"**\n",
    "   - First, measure current performance\n",
    "   - Identify bottleneck (retrieval or generation?)\n",
    "   - Try hybrid search, reranking, better embeddings\n",
    "   - Improve prompts and chunking strategy\n",
    "   - A/B test changes with real users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "print(\"\\nðŸ§¹ Cleaning up...\")\n",
    "rag.vector_store.delete_collection()\n",
    "print(\"âœ“ Done!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAG EVALUATION & OPTIMIZATION COMPLETE! ðŸŽ‰\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYou now have a production-ready RAG system!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review evaluation.md for detailed metrics\")\n",
    "print(\"2. Read interview_questions.md to prepare for interviews\")\n",
    "print(\"3. Practice explaining this system to others\")\n",
    "print(\"4. Try building RAG for your own use case!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
